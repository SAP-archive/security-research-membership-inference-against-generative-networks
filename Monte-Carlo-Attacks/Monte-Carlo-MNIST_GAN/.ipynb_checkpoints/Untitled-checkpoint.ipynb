{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least two variables have the same name: discrim_W1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fcc3147051e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/mnist_gan.ckpt-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_no\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.meta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_eager_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[0;32m   1282\u001b[0m           \u001b[0mrestore_sequentially\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restore_sequentially\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m           build_save=build_save, build_restore=build_restore)\n\u001b[0m\u001b[0;32m   1285\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m       \u001b[1;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[1;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[0;32m    741\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Graph mode needs to build save and restore together.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m     \u001b[0msaveables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ValidateAndSliceInputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_ValidateAndSliceInputs\u001b[1;34m(self, names_to_saveables)\u001b[0m\n\u001b[0;32m    594\u001b[0m     \"\"\"\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m       \u001b[0mnames_to_saveables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpListToDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[0msaveables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mOpListToDict\u001b[1;34m(op_list, convert_variable_to_tensor)\u001b[0m\n\u001b[0;32m    559\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m             raise ValueError(\"At least two variables have the same name: %s\" %\n\u001b[1;32m--> 561\u001b[1;33m                              name)\n\u001b[0m\u001b[0;32m    562\u001b[0m           \u001b[0mnames_to_saveables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: At least two variables have the same name: discrim_W1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from model import *\n",
    "from load import load_mnist_with_valid_set\n",
    "import time \n",
    "import scipy\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.0002\n",
    "batch_size = 128\n",
    "image_shape = [28,28,1]\n",
    "dim_z = 100\n",
    "dim_W1 = 1024\n",
    "dim_W2 = 128\n",
    "dim_W3 = 64\n",
    "dim_channel = 1\n",
    "\n",
    "# requirements\n",
    "# /vis folder\n",
    "# /data folder\n",
    "# /models folder\n",
    "# pip install pillow\n",
    "# pip install scikit-image\n",
    "# pip install scikit-learn\n",
    "\n",
    "# to run:\n",
    "# source activate tensorflow_p36 && python train_small.py 0.01 && python MC_Attacks_MNIST.py 10000 500 1 10 && sudo shutdown -P now\n",
    "# source activate tensorflow_p36 && python train_small.py 0.1 && python MC_Attacks_MNIST.py 10000 500 1 10 && sudo shutdown -P now\n",
    "\n",
    "# LOCAL:\n",
    "# python MC_Attacks_MNIST.py 1000 500 1 1\n",
    "# AMI: \n",
    "# python MC_Attacks_MNIST.py 10000 500 1 5\n",
    "\n",
    "mc_sample_size = 100 #usually 10.000 samples per batch\n",
    "model_no = '500' # which model to attack\n",
    "instance_no = 33\n",
    "exp_nos = int(1) # how many different experiments ofr specific indexes\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "experiment = 'MC_Attacks_MNIST'\n",
    "\n",
    "# only give the DCGAN 10% of training data\n",
    "train_inds = np.loadtxt('train_inds.csv').astype(int)\n",
    "percentage = np.loadtxt('percentage.csv')\n",
    "trX, vaX, teX, trY, vaY, teY = load_mnist_with_valid_set(train_inds, percentage=percentage, data_dir=data_dir)\n",
    "\n",
    "dcgan_model = DCGAN(\n",
    "        batch_size=batch_size,\n",
    "        image_shape=image_shape,\n",
    "        dim_z=dim_z,\n",
    "        dim_W1=dim_W1,\n",
    "        dim_W2=dim_W2,\n",
    "        dim_W3=dim_W3,\n",
    "        )\n",
    "\n",
    "Z_tf, Y_tf, image_tf, d_cost_tf, g_cost_tf, p_real, p_gen = dcgan_model.build_model()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver = tf.train.import_meta_graph('models/mnist_gan.ckpt-'+model_no+'.meta')\n",
    "\n",
    "saver.restore(sess, './models/mnist_gan.ckpt-'+model_no)\n",
    "\n",
    "dt = np.dtype([('instance_no', int),\n",
    "               ('exp_no', int),\n",
    "               ('method', int), # 1 = white box, 2 = euclidean_PCA, 3 = hog, 4 = euclidean_PCA category, 5 = hog category, 6 = ais\n",
    "               ('pca_n', int),\n",
    "               ('percentage_of_data', float),\n",
    "               ('percentile', float),\n",
    "               ('mc_euclidean_no_batches', int), # stuff\n",
    "               ('mc_hog_no_batches', int), # stuff\n",
    "               ('sigma_ais', float),\n",
    "               ('11_perc_mc_attack_log', float),\n",
    "               ('11_perc_mc_attack_eps', float),\n",
    "               ('11_perc_mc_attack_frac', float), \n",
    "               ('50_perc_mc_attack_log', float), \n",
    "               ('50_perc_mc_attack_eps', float),\n",
    "               ('50_perc_mc_attack_frac', float),\n",
    "               ('50_perc_white_box', float),\n",
    "               ('11_perc_white_box', float),\n",
    "               ('50_perc_ais', float),\n",
    "               ('50_perc_ais_acc_rate', float),\n",
    "              ])\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "def OneHot(X, n=10, negative_class=0.):\n",
    "    X = np.asarray(X).flatten()\n",
    "    Xoh = np.ones((len(X), n)) * negative_class\n",
    "    Xoh[np.arange(len(X)), X] = 1.\n",
    "    return Xoh\n",
    "\n",
    "# random numbers\n",
    "def generate_samples(sample_size=100):\n",
    "    Z_np_sample = np.random.randn(sample_size, dim_z)\n",
    "    Y_np_sample = OneHot( np.random.randint(10, size=[sample_size]))\n",
    "\n",
    "    Z_tf_sample, Y_tf_sample, image_tf_sample = dcgan_model.samples_generator(batch_size=sample_size)\n",
    "\n",
    "    generated_samples = sess.run(\n",
    "                    image_tf_sample,\n",
    "                    feed_dict={\n",
    "                        Z_tf_sample:Z_np_sample,\n",
    "                        Y_tf_sample:Y_np_sample\n",
    "                        })\n",
    "    generated_samples = (generated_samples + 1.)/2.\n",
    "    return generated_samples\n",
    "\n",
    "# indexes 1,11,21,31,... are ones, 2,12,22 are twos etc.\n",
    "def generate_samples_for_digits(sample_size=100):\n",
    "    \n",
    "    Z_np_sample = np.random.randn(sample_size, dim_z)\n",
    "    \n",
    "    digits = np.zeros((sample_size,)).astype(int)\n",
    "    for i in range(len(digits)):\n",
    "        digits[i] = i%10\n",
    "    Y_np_sample = OneHot( digits)\n",
    "\n",
    "    Z_tf_sample, Y_tf_sample, image_tf_sample = dcgan_model.samples_generator(batch_size=sample_size)\n",
    "\n",
    "    generated_samples = sess.run(\n",
    "                    image_tf_sample,\n",
    "                    feed_dict={\n",
    "                        Z_tf_sample:Z_np_sample,\n",
    "                        Y_tf_sample:Y_np_sample\n",
    "                        })\n",
    "    generated_samples = (generated_samples + 1.)/2.\n",
    "    return generated_samples\n",
    "\n",
    "def print_elapsed_time():\n",
    "    end_time = int(time.time())\n",
    "    d = divmod(end_time-start_time,86400)  # days\n",
    "    h = divmod(d[1],3600)  # hours\n",
    "    m = divmod(h[1],60)  # minutes\n",
    "    s = m[1]  # seconds\n",
    "\n",
    "    print('Elapsed Time: %d days, %d hours, %d minutes, %d seconds' % (d[0],h[0],m[0],s))\n",
    "\n",
    "def discriminate_for_wb(data_to_be_discriminated, labels_to_be_discriminated, training_indicator):\n",
    "    disc_results = np.zeros((len(data_to_be_discriminated),2))\n",
    "    \n",
    "    data_to_be_discriminated = data_to_be_discriminated.reshape( [-1, 28, 28, 1]) / 255\n",
    "    \n",
    "    disc_results[:,1] = training_indicator\n",
    "    \n",
    "    for iteration in range(len(data_to_be_discriminated) // batch_size):\n",
    "        start = iteration*batch_size\n",
    "        end = (iteration+1)*batch_size\n",
    "        \n",
    "        ind = np.arange(start,end)\n",
    "        Xs = tf.to_float(data_to_be_discriminated[ind].reshape( [-1, 28, 28, 1]) / 255)\n",
    "        Ys = tf.to_float(OneHot(labels_to_be_discriminated[ind]))\n",
    "        disc_results[ind, 0] = np.reshape(sess.run(dcgan_model.discriminate(Xs ,Ys)),(batch_size,))\n",
    "    \n",
    "    # fill last few elements\n",
    "    ind = np.arange(len(data_to_be_discriminated)-batch_size,len(data_to_be_discriminated))\n",
    "    Xs = tf.to_float(data_to_be_discriminated[ind].reshape( [-1, 28, 28, 1]) / 255)\n",
    "    Ys = tf.to_float(OneHot(labels_to_be_discriminated[ind]))\n",
    "    disc_results[ind, 0] = np.reshape(sess.run(dcgan_model.discriminate(Xs ,Ys)),(batch_size,))\n",
    "    \n",
    "    return disc_results\n",
    "\n",
    "def wb_attack_sample(disc_results_train, disc_results_validate):\n",
    "    results = np.concatenate((disc_results_train,disc_results_validate))\n",
    "    np.random.shuffle(results)\n",
    "    results = results[results[:,0].argsort()]\n",
    "\n",
    "    return results[-len(disc_results_train):,1].mean()\n",
    "\n",
    "def wb_attack(trX_inds, vaX_inds, exp_no):\n",
    "\n",
    "    disc_results_train = discriminate_for_wb(trX[trX_inds],trY[trX_inds],1)\n",
    "    disc_results_validate = discriminate_for_wb(vaX[vaX_inds],vaY[vaX_inds],0)\n",
    "\n",
    "    fifty_perc_wb_attack = wb_attack_sample(disc_results_train, disc_results_validate)\n",
    "\n",
    "    iterations = 1000\n",
    "    results_attacks = np.zeros((iterations, ))\n",
    "\n",
    "    for i in range(len(results_attacks)):\n",
    "        np.random.shuffle(disc_results_train)\n",
    "        results_attacks[i] = wb_attack_sample(disc_results_train[0:10], disc_results_validate)\n",
    "\n",
    "    eleven_perc_wb_attack = results_attacks.mean()\n",
    "\n",
    "    print('50_perc_wb_attack: %.3f'%(fifty_perc_wb_attack))\n",
    "    print('11_perc_wb_attack: %.3f'%(eleven_perc_wb_attack))\n",
    "\n",
    "    # white box\n",
    "    new_row = np.zeros(1, dtype = dt)[0]\n",
    "    new_row['instance_no'] = instance_no\n",
    "    new_row['exp_no'] = exp_no\n",
    "    new_row['method'] = 1 # white box\n",
    "    new_row['percentage_of_data'] = percentage\n",
    "    new_row['50_perc_white_box'] = fifty_perc_wb_attack\n",
    "    new_row['11_perc_white_box'] = eleven_perc_wb_attack\n",
    "    experiment_results.append(new_row)\n",
    "    np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "def calculate_results_matrices(distances_real_vs_sample,distances_real_vs_train, d_min=0.1):\n",
    "\n",
    "    results_sample = np.zeros((len(distances_real_vs_sample),4))\n",
    "    for i in range(len(results_sample)):\n",
    "        # indicate that dataset is a sample\n",
    "        results_sample[i][0] = 0\n",
    "        \n",
    "        integral_approx = 0\n",
    "        integral_approx_log = 0\n",
    "        integral_approx_eps = 0\n",
    "        for eps in distances_real_vs_sample[i]:\n",
    "            if eps < d_min:\n",
    "                integral_approx = integral_approx + d_min/eps\n",
    "                integral_approx_log = integral_approx_log + (-np.log(eps/d_min))\n",
    "                integral_approx_eps = integral_approx_eps + 1\n",
    "\n",
    "        integral_approx = integral_approx/len(distances_real_vs_sample[0])\n",
    "        integral_approx_log = integral_approx_log/len(distances_real_vs_sample[0])\n",
    "        integral_approx_eps = integral_approx_eps/len(distances_real_vs_sample[0])\n",
    "\n",
    "        results_sample[i][1] = integral_approx_log\n",
    "        results_sample[i][2] = integral_approx_eps\n",
    "        results_sample[i][3] = integral_approx\n",
    "\n",
    "    results_train = np.zeros((len(distances_real_vs_train),4))\n",
    "    for i in range(len(results_train)):\n",
    "        # indicate that dataset is a training data set\n",
    "        results_train[i][0] = 1\n",
    "        \n",
    "        integral_approx = 0\n",
    "        integral_approx_log = 0\n",
    "        integral_approx_eps = 0\n",
    "        for eps in distances_real_vs_train[i]:\n",
    "            if eps < d_min:\n",
    "                integral_approx = integral_approx + d_min/eps\n",
    "                integral_approx_log = integral_approx_log + (-np.log(eps/d_min))\n",
    "                integral_approx_eps = integral_approx_eps + 1\n",
    "\n",
    "        integral_approx = integral_approx/len(distances_real_vs_train[0])\n",
    "        integral_approx_log = integral_approx_log/len(distances_real_vs_train[0])\n",
    "        integral_approx_eps = integral_approx_eps/len(distances_real_vs_train[0])\n",
    "\n",
    "        results_train[i][1] = integral_approx_log\n",
    "        results_train[i][2] = integral_approx_eps\n",
    "        results_train[i][3] = integral_approx\n",
    "        \n",
    "    return results_sample,results_train\n",
    "\n",
    "def mc_attack_sample(results_sample, results_train):\n",
    "    results = np.concatenate((results_sample, results_train))\n",
    "    np.random.shuffle(results)\n",
    "    mc_attack_log = results[results[:,1].argsort()][:,0][-len(results_train):].mean()\n",
    "    np.random.shuffle(results)\n",
    "    mc_attack_eps = results[results[:,2].argsort()][:,0][-len(results_train):].mean()\n",
    "    np.random.shuffle(results)\n",
    "    mc_attack_frac = results[results[:,3].argsort()][:,0][-len(results_train):].mean()\n",
    "\n",
    "    return mc_attack_log, mc_attack_eps, mc_attack_frac\n",
    "\n",
    "def mc_attack(results_sample, results_train):\n",
    "\n",
    "    mc_attack_log, mc_attack_eps, mc_attack_frac = mc_attack_sample(results_sample, results_train)\n",
    "\n",
    "    print('50_perc_mc_attack_log: %.3f'%(mc_attack_log))\n",
    "    print('50_perc_mc_attack_eps: %.3f'%(mc_attack_eps))\n",
    "    print('50_perc_mc_attack_frac: %.3f'%(mc_attack_frac))\n",
    "\n",
    "    iterations = 1000\n",
    "    results_attacks = np.zeros((iterations, 3))\n",
    "\n",
    "    for i in range(len(results_attacks)):\n",
    "        np.random.shuffle(results_train)\n",
    "        results_attacks[i] = mc_attack_sample(results_sample, results_train[0:10])\n",
    "\n",
    "    print('11_perc_mc_attack_log: %.3f'%(results_attacks[:,0].mean()))\n",
    "    print('11_perc_mc_attack_eps: %.3f'%(results_attacks[:,1].mean()))\n",
    "    print('11_perc_mc_attack_frac: %.3f'%(results_attacks[:,2].mean()))\n",
    "\n",
    "    return mc_attack_log, mc_attack_eps, mc_attack_frac, results_attacks[:,0].mean(), results_attacks[:,1].mean(), results_attacks[:,2].mean()\n",
    "\n",
    "def euclidean_PCA_mc_attack_category(n_components_pca, trX_inds, vaX_inds, exp_no, mc_euclidean_no_batches, percentiles):\n",
    "    pca = PCA(n_components=n_components_pca)\n",
    "\n",
    "    pca.fit_transform(teX.reshape((len(teX),784)))\n",
    "\n",
    "    euclidean_trX = np.reshape(trX, (len(trX),784,))\n",
    "    euclidean_trX = euclidean_trX[trX_inds]\n",
    "    euclidean_trX = pca.transform(euclidean_trX)\n",
    "\n",
    "    euclidean_vaX = np.reshape(vaX, (len(vaX),784,))\n",
    "    euclidean_vaX = euclidean_vaX[vaX_inds]\n",
    "    euclidean_vaX = pca.transform(euclidean_vaX)\n",
    "\n",
    "    distances_trX = np.zeros((len(euclidean_trX), mc_euclidean_no_batches*mc_sample_size))\n",
    "    distances_vaX = np.zeros((len(euclidean_vaX), mc_euclidean_no_batches*mc_sample_size))\n",
    "\n",
    "    for i in range(mc_euclidean_no_batches):\n",
    "\n",
    "        print('Working on %d/%d'%(i, mc_euclidean_no_batches))\n",
    "\n",
    "        euclidean_generated_samples = generate_samples(mc_sample_size)\n",
    "\n",
    "        euclidean_generated_samples = euclidean_generated_samples - euclidean_generated_samples.min()\n",
    "        euclidean_generated_samples = euclidean_generated_samples*255/euclidean_generated_samples.max()\n",
    "        euclidean_generated_samples = np.reshape(euclidean_generated_samples, (len(euclidean_generated_samples),784,))\n",
    "        euclidean_generated_samples = pca.transform(euclidean_generated_samples)\n",
    "        \n",
    "        distances_trX_partial = np.zeros((len(euclidean_trX), mc_sample_size/10))\n",
    "        distances_vaX_partial = np.zeros((len(euclidean_vaX), mc_sample_size/10))\n",
    "        \n",
    "        for j in range(len(distances_trX_partial)):\n",
    "            # digit of current training example\n",
    "            digit = trY[trX_inds[j]]\n",
    "            # only compare to current digit\n",
    "            distances_trX_partial[j,:] = scipy.spatial.distance.cdist(euclidean_trX[j], euclidean_generated_samples[[digit+10*i for i in range(mc_sample_size//10)]], 'euclidean')\n",
    "        \n",
    "        for j in range(len(distances_vaX_partial)):\n",
    "            # digit of current training example\n",
    "            digit = vaY[vaX_inds[j]]\n",
    "            # only compare to current digit\n",
    "            distances_vaX_partial[j,:] = scipy.spatial.distance.cdist(euclidean_vaX[j], euclidean_generated_samples[[digit+10*i for i in range(mc_sample_size//10)]], 'euclidean')\n",
    "        \n",
    "        #distances_trX_partial = scipy.spatial.distance.cdist(euclidean_trX, euclidean_generated_samples, 'euclidean')\n",
    "        #distances_vaX_partial = scipy.spatial.distance.cdist(euclidean_vaX, euclidean_generated_samples, 'euclidean')\n",
    "\n",
    "        # optimized, better than concatenate\n",
    "        distances_trX[:,i*mc_sample_size:(i+1)*mc_sample_size] = distances_trX_partial\n",
    "        distances_vaX[:,i*mc_sample_size:(i+1)*mc_sample_size] = distances_vaX_partial\n",
    "        \n",
    "        print_elapsed_time()\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        print_elapsed_time()\n",
    "        print('Calculating Results Matrices for '+str(percentile)+' Percentile...')\n",
    "\n",
    "        d_min = np.percentile(np.concatenate((distances_trX,distances_vaX)),percentile)\n",
    "        results_sample,results_train = calculate_results_matrices(distances_vaX, distances_trX,d_min)\n",
    "        \n",
    "        # save data\n",
    "        new_row = np.zeros(1, dtype = dt)[0]\n",
    "        new_row['instance_no'] = instance_no\n",
    "        new_row['exp_no'] = exp_no\n",
    "        new_row['method'] = 4 # euclidean PCA cat\n",
    "        new_row['pca_n'] = n_components_pca\n",
    "        new_row['percentage_of_data'] = percentage\n",
    "        new_row['percentile'] = percentile\n",
    "        new_row['mc_euclidean_no_batches'] = mc_euclidean_no_batches\n",
    "\n",
    "        mc_attack_results = mc_attack(results_sample, results_train)\n",
    "        new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "        new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "        new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "        new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "        new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "        new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "        \n",
    "        experiment_results.append(new_row)\n",
    "        np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "def generate_batch_hog_features(samples):\n",
    "    features_matrix = np.zeros((len(samples),81))\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        features_matrix[i] = hog(samples[i].reshape((28, 28)), orientations=9, pixels_per_cell=(9, 9), visualise=False) #, transform_sqrt=True, block_norm='L2-Hys')\n",
    "    \n",
    "    return features_matrix\n",
    "\n",
    "def hog_mc_attack(trX_inds, vaX_inds, exp_no, mc_hog_no_batches, percentiles):\n",
    "\n",
    "    feature_matrix_vaX = generate_batch_hog_features(vaX[vaX_inds])\n",
    "    feature_matrix_trX = generate_batch_hog_features(trX[trX_inds])\n",
    "\n",
    "    distances_trX = np.zeros((len(feature_matrix_trX), mc_hog_no_batches*mc_sample_size))\n",
    "    distances_vaX = np.zeros((len(feature_matrix_vaX), mc_hog_no_batches*mc_sample_size))\n",
    "\n",
    "    for i in range(mc_hog_no_batches):\n",
    "\n",
    "        print('Working on %d/%d'%(i, mc_hog_no_batches))\n",
    "\n",
    "        generated_samples = generate_samples(mc_sample_size)\n",
    "\n",
    "        generated_samples = generated_samples - generated_samples.min()\n",
    "        generated_samples = generated_samples*255/generated_samples.max()\n",
    "\n",
    "        feature_matrix_generated = generate_batch_hog_features(generated_samples)\n",
    "\n",
    "        distances_trX_partial = scipy.spatial.distance.cdist(feature_matrix_trX, feature_matrix_generated, 'euclidean')\n",
    "        distances_vaX_partial = scipy.spatial.distance.cdist(feature_matrix_vaX, feature_matrix_generated, 'euclidean')\n",
    "\n",
    "        # optimized, better than concatenate\n",
    "        distances_trX[:,i*mc_sample_size:(i+1)*mc_sample_size] = distances_trX_partial\n",
    "        distances_vaX[:,i*mc_sample_size:(i+1)*mc_sample_size] = distances_vaX_partial\n",
    "\n",
    "        print_elapsed_time()\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        print_elapsed_time()\n",
    "        print('Calculating Results Matrices for '+str(percentile)+' Percentile...')\n",
    "\n",
    "        d_min = np.percentile(np.concatenate((distances_trX,distances_vaX)),percentile)\n",
    "        results_sample,results_train = calculate_results_matrices(distances_vaX, distances_trX,d_min)\n",
    "\n",
    "        # save data\n",
    "        new_row = np.zeros(1, dtype = dt)[0]\n",
    "        new_row['instance_no'] = instance_no\n",
    "        new_row['exp_no'] = exp_no\n",
    "        new_row['method'] = 3\n",
    "        new_row['percentage_of_data'] = percentage\n",
    "        new_row['percentile'] = percentile\n",
    "        new_row['mc_hog_no_batches'] = mc_hog_no_batches\n",
    "\n",
    "        mc_attack_results = mc_attack(results_sample, results_train)\n",
    "        new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "        new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "        new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "        new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "        new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "        new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "        \n",
    "        experiment_results.append(new_row)\n",
    "        np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "start_time = int(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0/100\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value gen_W4_1\n\t [[Node: gen_W4_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@gen_W4_1\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gen_W4_1)]]\n\nCaused by op 'gen_W4_1/read', defined at:\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-fcc3147051e3>\", line 58, in <module>\n    dim_W3=dim_W3,\n  File \"C:\\Users\\D065042\\OneDrive - SAP SE\\5. Praxisphase Bachelorarbeit\\Python\\Monte-Carlo-MNIST_AIS\\model.py\", line 65, in __init__\n    self.gen_W4 = tf.Variable(tf.random_normal([5,5,dim_channel,dim_W3+dim_y], stddev=0.02), name='gen_W4')\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 229, in __init__\n    constraint=constraint)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 376, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 127, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2728, in identity\n    \"Identity\", input=input, name=name)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value gen_W4_1\n\t [[Node: gen_W4_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@gen_W4_1\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gen_W4_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value gen_W4_1\n\t [[Node: gen_W4_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@gen_W4_1\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gen_W4_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a4fdab378e37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# euclidean pca mc attack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0meuclidean_PCA_mc_attack_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrX_inds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvaX_inds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc_euclidean_no_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m': Finished PCA Monte Carlo in experiment %d of %d'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_no\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_nos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-fcc3147051e3>\u001b[0m in \u001b[0;36meuclidean_PCA_mc_attack_category\u001b[1;34m(n_components_pca, trX_inds, vaX_inds, exp_no, mc_euclidean_no_batches, percentiles)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Working on %d/%d'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc_euclidean_no_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0meuclidean_generated_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmc_sample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0meuclidean_generated_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meuclidean_generated_samples\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meuclidean_generated_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-fcc3147051e3>\u001b[0m in \u001b[0;36mgenerate_samples\u001b[1;34m(sample_size)\u001b[0m\n\u001b[0;32m    108\u001b[0m                     feed_dict={\n\u001b[0;32m    109\u001b[0m                         \u001b[0mZ_tf_sample\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mZ_np_sample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                         \u001b[0mY_tf_sample\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY_np_sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m                         })\n\u001b[0;32m    112\u001b[0m     \u001b[0mgenerated_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgenerated_samples\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value gen_W4_1\n\t [[Node: gen_W4_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@gen_W4_1\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gen_W4_1)]]\n\nCaused by op 'gen_W4_1/read', defined at:\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-fcc3147051e3>\", line 58, in <module>\n    dim_W3=dim_W3,\n  File \"C:\\Users\\D065042\\OneDrive - SAP SE\\5. Praxisphase Bachelorarbeit\\Python\\Monte-Carlo-MNIST_AIS\\model.py\", line 65, in __init__\n    self.gen_W4 = tf.Variable(tf.random_normal([5,5,dim_channel,dim_W3+dim_y], stddev=0.02), name='gen_W4')\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 229, in __init__\n    constraint=constraint)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 376, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 127, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2728, in identity\n    \"Identity\", input=input, name=name)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value gen_W4_1\n\t [[Node: gen_W4_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@gen_W4_1\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](gen_W4_1)]]\n"
     ]
    }
   ],
   "source": [
    "mc_hog_no_batches = 100\n",
    "mc_euclidean_no_batches = 100\n",
    "\n",
    "for exp_no in range(exp_nos):\n",
    "\n",
    "    trX_inds = np.arange(len(trX))\n",
    "    np.random.shuffle(trX_inds)\n",
    "    trX_inds = trX_inds[0:100]\n",
    "\n",
    "    vaX_inds = np.arange(len(trX))\n",
    "    np.random.shuffle(vaX_inds)\n",
    "    vaX_inds = vaX_inds[0:100]\n",
    "\n",
    "    # euclidean pca mc attack\n",
    "    euclidean_PCA_mc_attack_category(42, trX_inds, vaX_inds, exp_no, mc_euclidean_no_batches, [1,0.1,0.01,0.001,0.0001])\n",
    "    print(experiment+': Finished PCA Monte Carlo in experiment %d of %d'%(exp_no+1, exp_nos))\n",
    "\n",
    "    print_elapsed_time()\n",
    "    \n",
    "print(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
