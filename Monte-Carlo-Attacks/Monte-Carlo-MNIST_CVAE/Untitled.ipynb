{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE THE NEW DATA !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from ./models/mnist_gan.ckpt-299\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "\n",
    "import ais\n",
    "import matplotlib.pyplot as plt\n",
    "from priors import NormalPrior\n",
    "from kernels import ParsenDensityEstimator\n",
    "from scipy.stats import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mnist_data\n",
    "import vae\n",
    "import plot_utils\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "\n",
    "\"\"\" parameters \"\"\"\n",
    "\n",
    "# source activate tensorflow_p36 && pip install pillow && pip install scikit-image && pip install scikit-learn\n",
    "# source activate tensorflow_p36 && python run_main.py --dim_z 10 --num_epochs 300\n",
    "# source activate tensorflow_p36 && python mc_attack_cvae.py 299 5 && python mc_attack_cvae.py 299 5 && sudo shutdown -P now\n",
    "\n",
    "# combined:\n",
    "# source activate tensorflow_p36 && pip install pillow && pip install scikit-image && pip install scikit-learn && python run_main.py --dim_z 10 --num_epochs 300 && python mc_attack_cvae.py 299 5 && python mc_attack_cvae.py 299 5 && sudo shutdown -P now\n",
    "# source activate tensorflow_p36 && python mc_attack_cvae.py 299 5 && python mc_attack_cvae.py 299 5 && sudo shutdown -P now\n",
    "# source activate tensorflow_p36 && python mc_attack_cvae.py 299 5 && python mc_attack_cvae.py 299 5 && sudo shutdown -P now\n",
    "model_no = '299' # which model to attack\n",
    "exp_nos = int(1) # how many different experiments ofr specific indexes\n",
    "\n",
    "instance_no = np.random.randint(10000)\n",
    "experiment = 'MC_ATTACK_CVAE' + str(instance_no)\n",
    "percentage = 0.1\n",
    "\n",
    "dt = np.dtype([('instance_no', int),\n",
    "               ('exp_no', int),\n",
    "               ('method', int), # 1 = white box, 2 = euclidean_PCA, 3 = hog, 4 = euclidean_PCA category, 5 = hog category, 6 = ais\n",
    "               ('pca_n', int),\n",
    "               ('percentage_of_data', float),\n",
    "               ('percentile', float),\n",
    "               ('mc_euclidean_no_batches', int), # stuff\n",
    "               ('mc_hog_no_batches', int), # stuff\n",
    "               ('sigma_ais', float),\n",
    "               ('11_perc_mc_attack_log', float),\n",
    "               ('11_perc_mc_attack_eps', float),\n",
    "               ('11_perc_mc_attack_frac', float), \n",
    "               ('50_perc_mc_attack_log', float), \n",
    "               ('50_perc_mc_attack_eps', float),\n",
    "               ('50_perc_mc_attack_frac', float),\n",
    "               ('50_perc_white_box', float),\n",
    "               ('11_perc_white_box', float),\n",
    "               ('50_perc_ais', float),\n",
    "               ('50_perc_ais_acc_rate', float),\n",
    "              ])\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "IMAGE_SIZE_MNIST = 28\n",
    "n_hidden = 500\n",
    "dim_img = IMAGE_SIZE_MNIST**2  # number of pixels for a MNIST image\n",
    "dim_z = 10\n",
    "model_no = '299'\n",
    "\n",
    "\"\"\" prepare MNIST data \"\"\"\n",
    "\n",
    "train_total_data, train_size, valid_total_data, validation_size, test_total_data, test_size, _, _ = mnist_data.prepare_MNIST_data(reuse=True)\n",
    "# compatibility with old attack\n",
    "vaY = np.where(valid_total_data[:,784:795] == 1)[1]\n",
    "trY = np.where(train_total_data[:,784:795] == 1)[1]\n",
    "teY = np.where(test_total_data[:,784:795] == 1)[1]\n",
    "vaX = valid_total_data[:,0:784]\n",
    "trX = train_total_data[:,0:784]\n",
    "teX = test_total_data[:,0:784]\n",
    "n_samples = train_size\n",
    "\n",
    "\"\"\" build graph \"\"\"\n",
    "\n",
    "# input placeholders\n",
    "# In denoising-autoencoder, x_hat == x + noise, otherwise x_hat == x\n",
    "x_hat = tf.placeholder(tf.float32, shape=[None, dim_img], name='input_img')\n",
    "x = tf.placeholder(tf.float32, shape=[None, dim_img], name='target_img')\n",
    "y = tf.placeholder(tf.float32, shape=[None, mnist_data.NUM_LABELS], name='target_labels')\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# input for PMLR\n",
    "z_in = tf.placeholder(tf.float32, shape=[None, dim_z], name='latent_variable')\n",
    "fack_id_in = tf.placeholder(tf.float32, shape=[None, mnist_data.NUM_LABELS], name='latent_variable')\n",
    "\n",
    "# network architecture\n",
    "x_, z, loss, neg_marginal_likelihood, KL_divergence = vae.autoencoder(x_hat, x, y, dim_img, dim_z, n_hidden, keep_prob)\n",
    "\n",
    "decoded = vae.decoder(z_in, fack_id_in, dim_img, n_hidden)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver = tf.train.import_meta_graph('models/mnist_gan.ckpt-'+model_no+'.meta')\n",
    "saver.restore(sess, './models/mnist_gan.ckpt-'+model_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0/3\n",
      "Elapsed Time: 0 days, 0 hours, 0 minutes, 1 seconds\n",
      "Working on 1/3\n",
      "Elapsed Time: 0 days, 0 hours, 0 minutes, 2 seconds\n",
      "Working on 2/3\n",
      "Elapsed Time: 0 days, 0 hours, 0 minutes, 2 seconds\n",
      "Calculating Results Matrices for flexible d_min...\n",
      "50_perc_mc_attack_log: 0.490\n",
      "50_perc_mc_attack_eps: 0.490\n",
      "50_perc_mc_attack_frac: 0.490\n",
      "successfull_set_attack_1: 0.000\n",
      "successfull_set_attack_2: 0.000\n",
      "successfull_set_attack_3: 0.000\n",
      "11_perc_mc_attack_log: 0.027\n",
      "11_perc_mc_attack_eps: 0.036\n",
      "11_perc_mc_attack_frac: 0.028\n",
      "Working on 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\skimage\\feature\\_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 0 days, 0 hours, 0 minutes, 3 seconds\n",
      "Working on 1/3\n",
      "Elapsed Time: 0 days, 0 hours, 0 minutes, 3 seconds\n",
      "Working on 2/3\n",
      "Elapsed Time: 0 days, 0 hours, 0 minutes, 4 seconds\n",
      "Calculating Results Matrices for flexible d_min...\n",
      "50_perc_mc_attack_log: 0.520\n",
      "50_perc_mc_attack_eps: 0.520\n",
      "50_perc_mc_attack_frac: 0.520\n",
      "successfull_set_attack_1: 1.000\n",
      "successfull_set_attack_2: 1.000\n",
      "successfull_set_attack_3: 1.000\n",
      "11_perc_mc_attack_log: 0.126\n",
      "11_perc_mc_attack_eps: 0.140\n",
      "11_perc_mc_attack_frac: 0.140\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mnist_data\n",
    "import os\n",
    "import vae\n",
    "import plot_utils\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.feature import hog\n",
    "\n",
    "def OneHot(X, n=10, negative_class=0.):\n",
    "    X = np.asarray(X).flatten()\n",
    "    if n is None:\n",
    "        n = np.max(X) + 1\n",
    "    Xoh = np.ones((len(X), n)) * negative_class\n",
    "    Xoh[np.arange(len(X)), X] = 1.\n",
    "    return Xoh\n",
    "\n",
    "# indexes 1,11,21,31,... are ones, 2,12,22 are twos etc.\n",
    "def generate_samples_for_digits(sample_size=100):\n",
    "    \n",
    "    Z_np_sample_buffer = np.random.randn(sample_size, dim_z)\n",
    "    \n",
    "    digits = np.zeros((sample_size,)).astype(int)\n",
    "    for i in range(len(digits)):\n",
    "        digits[i] = i%10\n",
    "    Y_np_sample = OneHot( digits)\n",
    "\n",
    "    generated_samples = sess.run(decoded, feed_dict={z_in: Z_np_sample_buffer, fack_id_in: Y_np_sample, keep_prob : 1})\n",
    "\n",
    "    if (np.any(np.isnan(generated_samples))) or (not np.all(np.isfinite(generated_samples))):\n",
    "        print('Problem')\n",
    "        print(generated_samples[0])\n",
    "        print(generated_samples[1])\n",
    "        generated_samples = generate_samples_for_digits(sample_size)\n",
    "\n",
    "    return generated_samples\n",
    "\n",
    "def print_elapsed_time():\n",
    "    end_time = int(time.time())\n",
    "    d = divmod(end_time-start_time,86400)  # days\n",
    "    h = divmod(d[1],3600)  # hours\n",
    "    m = divmod(h[1],60)  # minutes\n",
    "    s = m[1]  # seconds\n",
    "\n",
    "    print('Elapsed Time: %d days, %d hours, %d minutes, %d seconds' % (d[0],h[0],m[0],s))\n",
    "\n",
    "def calculate_results_matrices(distances_real_vs_sample,distances_real_vs_train, d_min=0.1):\n",
    "\n",
    "    results_sample = np.zeros((len(distances_real_vs_sample),4))\n",
    "    for i in range(len(results_sample)):\n",
    "        # indicate that dataset is a sample\n",
    "        results_sample[i][0] = 0\n",
    "        \n",
    "        integral_approx = 0\n",
    "        integral_approx_log = 0\n",
    "        integral_approx_eps = 0\n",
    "        for eps in distances_real_vs_sample[i]:\n",
    "            if eps < d_min:\n",
    "                integral_approx = integral_approx + d_min/eps\n",
    "                integral_approx_log = integral_approx_log + (-np.log(eps/d_min))\n",
    "                integral_approx_eps = integral_approx_eps + 1\n",
    "\n",
    "        integral_approx = integral_approx/len(distances_real_vs_sample[0])\n",
    "        integral_approx_log = integral_approx_log/len(distances_real_vs_sample[0])\n",
    "        integral_approx_eps = integral_approx_eps/len(distances_real_vs_sample[0])\n",
    "\n",
    "        results_sample[i][1] = integral_approx_log\n",
    "        results_sample[i][2] = integral_approx_eps\n",
    "        results_sample[i][3] = integral_approx\n",
    "\n",
    "    results_train = np.zeros((len(distances_real_vs_train),4))\n",
    "    for i in range(len(results_train)):\n",
    "        # indicate that dataset is a training data set\n",
    "        results_train[i][0] = 1\n",
    "        \n",
    "        integral_approx = 0\n",
    "        integral_approx_log = 0\n",
    "        integral_approx_eps = 0\n",
    "        for eps in distances_real_vs_train[i]:\n",
    "            if eps < d_min:\n",
    "                integral_approx = integral_approx + d_min/eps\n",
    "                integral_approx_log = integral_approx_log + (-np.log(eps/d_min))\n",
    "                integral_approx_eps = integral_approx_eps + 1\n",
    "\n",
    "        integral_approx = integral_approx/len(distances_real_vs_train[0])\n",
    "        integral_approx_log = integral_approx_log/len(distances_real_vs_train[0])\n",
    "        integral_approx_eps = integral_approx_eps/len(distances_real_vs_train[0])\n",
    "\n",
    "        results_train[i][1] = integral_approx_log\n",
    "        results_train[i][2] = integral_approx_eps\n",
    "        results_train[i][3] = integral_approx\n",
    "        \n",
    "    return results_sample,results_train\n",
    "\n",
    "def mc_attack_sample(results_sample, results_train):\n",
    "    results = np.concatenate((results_sample, results_train))\n",
    "    np.random.shuffle(results)\n",
    "    mc_attack_log = results[results[:,1].argsort()][:,0][-len(results_train):].mean()\n",
    "    np.random.shuffle(results)\n",
    "    mc_attack_eps = results[results[:,2].argsort()][:,0][-len(results_train):].mean()\n",
    "    np.random.shuffle(results)\n",
    "    mc_attack_frac = results[results[:,3].argsort()][:,0][-len(results_train):].mean()\n",
    "\n",
    "    successfull_set_attack_1 = results_train[:,1].sum() > results_sample[:,1].sum()\n",
    "    successfull_set_attack_2 = results_train[:,2].sum() > results_sample[:,2].sum()\n",
    "    successfull_set_attack_3 = results_train[:,3].sum() > results_sample[:,3].sum()\n",
    "\n",
    "    return mc_attack_log, mc_attack_eps, mc_attack_frac, successfull_set_attack_1, successfull_set_attack_2, successfull_set_attack_3\n",
    "\n",
    "def mc_attack(results_sample, results_train):\n",
    "\n",
    "    mc_attack_log, mc_attack_eps, mc_attack_frac, successfull_set_attack_1, successfull_set_attack_2, successfull_set_attack_3 = mc_attack_sample(results_sample, results_train)\n",
    "\n",
    "    print('50_perc_mc_attack_log: %.3f'%(mc_attack_log))\n",
    "    print('50_perc_mc_attack_eps: %.3f'%(mc_attack_eps))\n",
    "    print('50_perc_mc_attack_frac: %.3f'%(mc_attack_frac))\n",
    "    print('successfull_set_attack_1: %.3f'%(successfull_set_attack_1))\n",
    "    print('successfull_set_attack_2: %.3f'%(successfull_set_attack_2))\n",
    "    print('successfull_set_attack_3: %.3f'%(successfull_set_attack_3))\n",
    "\n",
    "    iterations = 1000\n",
    "    results_attacks = np.zeros((iterations, 3))\n",
    "\n",
    "    for i in range(len(results_attacks)):\n",
    "        np.random.shuffle(results_train)\n",
    "        res = mc_attack_sample(results_sample, results_train[0:10])\n",
    "        results_attacks[i][0] = res[0]\n",
    "        results_attacks[i][1] = res[1]\n",
    "        results_attacks[i][2] = res[2]\n",
    "\n",
    "    print('11_perc_mc_attack_log: %.3f'%(results_attacks[:,0].mean()))\n",
    "    print('11_perc_mc_attack_eps: %.3f'%(results_attacks[:,1].mean()))\n",
    "    print('11_perc_mc_attack_frac: %.3f'%(results_attacks[:,2].mean()))\n",
    "\n",
    "    return mc_attack_log, mc_attack_eps, mc_attack_frac, results_attacks[:,0].mean(), results_attacks[:,1].mean(), results_attacks[:,2].mean(), successfull_set_attack_1, successfull_set_attack_2, successfull_set_attack_3\n",
    "\n",
    "def euclidean_PCA_mc_attack(n_components_pca, trX_inds, vaX_inds, exp_no, mc_euclidean_no_batches, mc_sample_size, percentiles):\n",
    "    pca = PCA(n_components=n_components_pca)\n",
    "\n",
    "    pca.fit_transform(teX.reshape((len(teX),784)))\n",
    "\n",
    "    euclidean_trX = np.reshape(trX, (len(trX),784,))\n",
    "    euclidean_trX = euclidean_trX[trX_inds]\n",
    "    euclidean_trX = pca.transform(euclidean_trX)\n",
    "\n",
    "    euclidean_vaX = np.reshape(vaX, (len(vaX),784,))\n",
    "    euclidean_vaX = euclidean_vaX[vaX_inds]\n",
    "    euclidean_vaX = pca.transform(euclidean_vaX)\n",
    "\n",
    "    distances_trX = np.zeros((len(euclidean_trX), mc_euclidean_no_batches*mc_sample_size))\n",
    "    distances_vaX = np.zeros((len(euclidean_vaX), mc_euclidean_no_batches*mc_sample_size))\n",
    "\n",
    "    for i in range(mc_euclidean_no_batches):\n",
    "\n",
    "        print('Working on %d/%d'%(i, mc_euclidean_no_batches))\n",
    "\n",
    "        euclidean_generated_samples = generate_samples_for_digits(mc_sample_size)\n",
    "\n",
    "        #euclidean_generated_samples = euclidean_generated_samples - euclidean_generated_samples.min()\n",
    "        #euclidean_generated_samples = euclidean_generated_samples*255/euclidean_generated_samples.max()\n",
    "        euclidean_generated_samples = np.reshape(euclidean_generated_samples, (len(euclidean_generated_samples),784,))\n",
    "        euclidean_generated_samples = pca.transform(euclidean_generated_samples)\n",
    "\n",
    "        distances_trX_partial = scipy.spatial.distance.cdist(euclidean_trX, euclidean_generated_samples, 'euclidean')\n",
    "        distances_vaX_partial = scipy.spatial.distance.cdist(euclidean_vaX, euclidean_generated_samples, 'euclidean')\n",
    "\n",
    "        # optimized, better than concatenate\n",
    "        distances_trX[:,i*mc_sample_size:(i+1)*mc_sample_size] = distances_trX_partial\n",
    "        distances_vaX[:,i*mc_sample_size:(i+1)*mc_sample_size] = distances_vaX_partial\n",
    "        \n",
    "        print_elapsed_time()\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        print_elapsed_time()\n",
    "        print('Calculating Results Matrices for '+str(percentile)+' Percentile...')\n",
    "\n",
    "        d_min = np.percentile(np.concatenate((distances_trX,distances_vaX)),percentile)\n",
    "        results_sample,results_train = calculate_results_matrices(distances_vaX, distances_trX,d_min)\n",
    "        \n",
    "        # save data\n",
    "        new_row = np.zeros(1, dtype = dt)[0]\n",
    "        new_row['instance_no'] = instance_no\n",
    "        new_row['exp_no'] = exp_no\n",
    "        new_row['method'] = 2 # euclidean PCA\n",
    "        new_row['pca_n'] = n_components_pca\n",
    "        new_row['percentage_of_data'] = percentage\n",
    "        new_row['percentile'] = percentile\n",
    "        new_row['mc_euclidean_no_batches'] = mc_euclidean_no_batches\n",
    "\n",
    "        mc_attack_results = mc_attack(results_sample, results_train)\n",
    "        new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "        new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "        new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "        new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "        new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "        new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "        \n",
    "        experiment_results.append(new_row)\n",
    "        np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "def euclidean_PCA_mc_attack_category(n_components_pca, trX_inds, vaX_inds, exp_no, mc_euclidean_no_batches, mc_sample_size, percentiles):\n",
    "    pca = PCA(n_components=n_components_pca)\n",
    "\n",
    "    pca.fit_transform(teX.reshape((len(teX),784)))\n",
    "\n",
    "    euclidean_trX = np.reshape(trX, (len(trX),784,))\n",
    "    euclidean_trX = euclidean_trX[trX_inds]\n",
    "    euclidean_trX = pca.transform(euclidean_trX)\n",
    "\n",
    "    euclidean_vaX = np.reshape(vaX, (len(vaX),784,))\n",
    "    euclidean_vaX = euclidean_vaX[vaX_inds]\n",
    "    euclidean_vaX = pca.transform(euclidean_vaX)\n",
    "\n",
    "    distances_trX = np.zeros((len(euclidean_trX), mc_euclidean_no_batches*mc_sample_size // 10))\n",
    "    distances_vaX = np.zeros((len(euclidean_vaX), mc_euclidean_no_batches*mc_sample_size // 10))\n",
    "\n",
    "    for i in range(mc_euclidean_no_batches):\n",
    "\n",
    "        print('Working on %d/%d'%(i, mc_euclidean_no_batches))\n",
    "\n",
    "        euclidean_generated_samples = generate_samples_for_digits(mc_sample_size)\n",
    "\n",
    "        euclidean_generated_samples = np.reshape(euclidean_generated_samples, (len(euclidean_generated_samples),784,))\n",
    "        euclidean_generated_samples = pca.transform(euclidean_generated_samples)\n",
    "        \n",
    "        for digit in range(10):\n",
    "            # indexes of 1's, 2's, 3's etc.\n",
    "            digit_indexes_train = np.where(trY[trX_inds] == digit)\n",
    "            digit_indexes_sample = [digit+10*i for i in range(mc_sample_size//10)]\n",
    "            # only compare to current digit\n",
    "            distances_trX[digit_indexes_train,i*mc_sample_size//10:(i+1)*mc_sample_size//10] = scipy.spatial.distance.cdist(euclidean_trX[digit_indexes_train], euclidean_generated_samples[digit_indexes_sample], 'euclidean')\n",
    "\n",
    "        for digit in range(10):\n",
    "            # indexes of 1's, 2's, 3's etc.\n",
    "            digit_indexes_va = np.where(vaY[vaX_inds] == digit)\n",
    "            digit_indexes_sample = [digit+10*i for i in range(mc_sample_size//10)]\n",
    "            # only compare to current digit\n",
    "            distances_vaX[digit_indexes_va,i*mc_sample_size//10:(i+1)*mc_sample_size//10] = scipy.spatial.distance.cdist(euclidean_vaX[digit_indexes_va], euclidean_generated_samples[digit_indexes_sample], 'euclidean')\n",
    "        \n",
    "        print_elapsed_time()\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        print_elapsed_time()\n",
    "        print('Calculating Results Matrices for '+str(percentile)+' Percentile...')\n",
    "\n",
    "        d_min = np.percentile(np.concatenate((distances_trX,distances_vaX)),percentile)\n",
    "        results_sample,results_train = calculate_results_matrices(distances_vaX, distances_trX,d_min)\n",
    "\n",
    "        # save data\n",
    "        new_row = np.zeros(1, dtype = dt)[0]\n",
    "        new_row['instance_no'] = instance_no\n",
    "        new_row['exp_no'] = exp_no\n",
    "        new_row['method'] = 4 # euclidean PCA cat\n",
    "        new_row['pca_n'] = n_components_pca\n",
    "        new_row['percentage_of_data'] = percentage\n",
    "        new_row['percentile'] = percentile\n",
    "        new_row['mc_euclidean_no_batches'] = mc_euclidean_no_batches\n",
    "\n",
    "        mc_attack_results = mc_attack(results_sample, results_train)\n",
    "        new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "        new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "        new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "        new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "        new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "        new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "        \n",
    "        experiment_results.append(new_row)\n",
    "        np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "    print('Calculating Results Matrices for flexible d_min...')\n",
    "    distances = np.concatenate((distances_trX,distances_vaX))\n",
    "    d_min = np.median([distances[i].min() for i in range(len(distances))])\n",
    "    results_sample,results_train = calculate_results_matrices(distances_vaX, distances_trX,d_min)\n",
    "\n",
    "    # save data\n",
    "    new_row = np.zeros(1, dtype = dt)[0]\n",
    "    new_row['instance_no'] = instance_no\n",
    "    new_row['exp_no'] = exp_no\n",
    "    new_row['method'] = 4 # euclidean PCA cat\n",
    "    new_row['pca_n'] = n_components_pca\n",
    "    new_row['percentage_of_data'] = percentage\n",
    "    new_row['percentile'] = -1 # dynamic\n",
    "    new_row['mc_euclidean_no_batches'] = mc_euclidean_no_batches\n",
    "    mc_attack_results = mc_attack(results_sample, results_train)\n",
    "    new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "    new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "    new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "    new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "    new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "    new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "    \n",
    "    experiment_results.append(new_row)\n",
    "    np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "    return results_sample,results_train\n",
    "\n",
    "def generate_batch_hog_features(samples):\n",
    "    features_matrix = np.zeros((len(samples),81))\n",
    "\n",
    "    for i in range(len(samples)):\n",
    "        features_matrix[i] = hog(samples[i].reshape((28, 28)), orientations=9, pixels_per_cell=(9, 9), visualise=False) #, transform_sqrt=True, block_norm='L2-Hys')\n",
    "    \n",
    "    return features_matrix\n",
    "\n",
    "def hog_mc_attack(trX_inds, vaX_inds, exp_no, mc_hog_no_batches, mc_sample_size, percentiles):\n",
    "\n",
    "    feature_matrix_vaX = generate_batch_hog_features(vaX[vaX_inds])\n",
    "    feature_matrix_trX = generate_batch_hog_features(trX[trX_inds])\n",
    "\n",
    "    distances_trX = np.zeros((len(feature_matrix_trX), mc_hog_no_batches*mc_sample_size))\n",
    "    distances_vaX = np.zeros((len(feature_matrix_vaX), mc_hog_no_batches*mc_sample_size))\n",
    "\n",
    "    for i in range(mc_hog_no_batches):\n",
    "\n",
    "        print('Working on %d/%d'%(i, mc_hog_no_batches))\n",
    "\n",
    "        generated_samples = generate_samples_for_digits(mc_sample_size)\n",
    "\n",
    "        generated_samples = generated_samples - generated_samples.min()\n",
    "        generated_samples = generated_samples*255/generated_samples.max()\n",
    "\n",
    "        feature_matrix_generated = generate_batch_hog_features(generated_samples)\n",
    "\n",
    "        distances_trX_partial = scipy.spatial.distance.cdist(feature_matrix_trX, feature_matrix_generated, 'euclidean')\n",
    "        distances_vaX_partial = scipy.spatial.distance.cdist(feature_matrix_vaX, feature_matrix_generated, 'euclidean')\n",
    "\n",
    "        # optimized, better than concatenate\n",
    "        distances_trX[:,i*mc_sample_size:(i+1)*mc_sample_size] = distances_trX_partial\n",
    "        distances_vaX[:,i*mc_sample_size:(i+1)*mc_sample_size] = distances_vaX_partial\n",
    "\n",
    "        print_elapsed_time()\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        print_elapsed_time()\n",
    "        print('Calculating Results Matrices for '+str(percentile)+' Percentile...')\n",
    "\n",
    "        d_min = np.percentile(np.concatenate((distances_trX,distances_vaX)),percentile)\n",
    "        results_sample,results_train = calculate_results_matrices(distances_vaX, distances_trX,d_min)\n",
    "\n",
    "        # save data\n",
    "        new_row = np.zeros(1, dtype = dt)[0]\n",
    "        new_row['instance_no'] = instance_no\n",
    "        new_row['exp_no'] = exp_no\n",
    "        new_row['method'] = 3\n",
    "        new_row['percentage_of_data'] = percentage\n",
    "        new_row['percentile'] = percentile\n",
    "        new_row['mc_hog_no_batches'] = mc_hog_no_batches\n",
    "\n",
    "        mc_attack_results = mc_attack(results_sample, results_train)\n",
    "        new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "        new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "        new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "        new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "        new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "        new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "        \n",
    "        experiment_results.append(new_row)\n",
    "        np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "def hog_mc_attack_category(trX_inds, vaX_inds, exp_no, mc_hog_no_batches, mc_sample_size, percentiles):\n",
    "\n",
    "    feature_matrix_vaX = generate_batch_hog_features(vaX[vaX_inds])\n",
    "    feature_matrix_trX = generate_batch_hog_features(trX[trX_inds])\n",
    "\n",
    "    distances_trX = np.zeros((len(feature_matrix_trX), mc_hog_no_batches*mc_sample_size // 10))\n",
    "    distances_vaX = np.zeros((len(feature_matrix_vaX), mc_hog_no_batches*mc_sample_size // 10))\n",
    "\n",
    "    for i in range(mc_hog_no_batches):\n",
    "\n",
    "        print('Working on %d/%d'%(i, mc_hog_no_batches))\n",
    "\n",
    "        generated_samples = generate_samples_for_digits(mc_sample_size)\n",
    "\n",
    "        generated_samples = generated_samples - generated_samples.min()\n",
    "        generated_samples = generated_samples*255/generated_samples.max()\n",
    "\n",
    "        feature_matrix_generated = generate_batch_hog_features(generated_samples)\n",
    "\n",
    "        for digit in range(10):\n",
    "            # indexes of 1's, 2's, 3's etc.\n",
    "            digit_indexes_train = np.where(trY[trX_inds] == digit)\n",
    "            digit_indexes_sample = [digit+10*i for i in range(mc_sample_size//10)]\n",
    "            # only compare to current digit\n",
    "            distances_trX[digit_indexes_train,i*mc_sample_size//10:(i+1)*mc_sample_size//10] = scipy.spatial.distance.cdist(feature_matrix_trX[digit_indexes_train], feature_matrix_generated[digit_indexes_sample], 'euclidean')\n",
    "\n",
    "        for digit in range(10):\n",
    "            # indexes of 1's, 2's, 3's etc.\n",
    "            digit_indexes_va = np.where(vaY[vaX_inds] == digit)\n",
    "            digit_indexes_sample = [digit+10*i for i in range(mc_sample_size//10)]\n",
    "            # only compare to current digit\n",
    "            distances_vaX[digit_indexes_va,i*mc_sample_size//10:(i+1)*mc_sample_size//10] = scipy.spatial.distance.cdist(feature_matrix_vaX[digit_indexes_va], feature_matrix_generated[digit_indexes_sample], 'euclidean')\n",
    "\n",
    "        print_elapsed_time()\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        print_elapsed_time()\n",
    "        print('Calculating Results Matrices for '+str(percentile)+' Percentile...')\n",
    "\n",
    "        d_min = np.percentile(np.concatenate((distances_trX,distances_vaX)),percentile)\n",
    "        results_sample,results_train = calculate_results_matrices(distances_vaX, distances_trX,d_min)\n",
    "\n",
    "        # save data\n",
    "        new_row = np.zeros(1, dtype = dt)[0]\n",
    "        new_row['instance_no'] = instance_no\n",
    "        new_row['exp_no'] = exp_no\n",
    "        new_row['method'] = 5 # hog cat\n",
    "        new_row['percentage_of_data'] = percentage\n",
    "        new_row['percentile'] = percentile\n",
    "        new_row['mc_hog_no_batches'] = mc_hog_no_batches\n",
    "\n",
    "        mc_attack_results = mc_attack(results_sample, results_train)\n",
    "        new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "        new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "        new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "        new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "        new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "        new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "        \n",
    "        experiment_results.append(new_row)\n",
    "        np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "    print('Calculating Results Matrices for flexible d_min...')\n",
    "    distances = np.concatenate((distances_trX,distances_vaX))\n",
    "    d_min = np.median([distances[i].min() for i in range(len(distances))])\n",
    "    results_sample,results_train = calculate_results_matrices(distances_vaX, distances_trX,d_min)\n",
    "\n",
    "    # save data\n",
    "    new_row = np.zeros(1, dtype = dt)[0]\n",
    "    new_row['instance_no'] = instance_no\n",
    "    new_row['exp_no'] = exp_no\n",
    "    new_row['method'] = 5 # hog cat\n",
    "    new_row['percentage_of_data'] = percentage\n",
    "    new_row['percentile'] = -1\n",
    "    new_row['mc_hog_no_batches'] = mc_hog_no_batches\n",
    "\n",
    "    mc_attack_results = mc_attack(results_sample, results_train)\n",
    "    new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "    new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "    new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "    new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "    new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "    new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "    \n",
    "    experiment_results.append(new_row)\n",
    "    np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))\n",
    "\n",
    "    return results_sample,results_train\n",
    "\n",
    "start_time = int(time.time())\n",
    "\n",
    "trX_inds = np.arange(len(trX))\n",
    "np.random.shuffle(trX_inds)\n",
    "trX_inds = trX_inds[0:100]\n",
    "\n",
    "vaX_inds = np.arange(len(trX))\n",
    "np.random.shuffle(vaX_inds)\n",
    "vaX_inds = vaX_inds[0:100]\n",
    "\n",
    "results_sample_pca,results_train_pca = euclidean_PCA_mc_attack_category(40, trX_inds, vaX_inds, 1, 3, 1000, [])\n",
    "results_sample_hog,results_train_hog = hog_mc_attack_category(trX_inds, vaX_inds, 1, 3, 1000, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_normalizer = np.max(np.concatenate((results_sample_pca,results_train_pca)))\n",
    "hog_normalizer = np.max(np.concatenate((results_sample_hog,results_train_hog)))\n",
    "results_sample_pca = results_sample_pca/pca_normalizer\n",
    "results_train_pca = results_train_pca/pca_normalizer\n",
    "results_sample_hog = results_sample_hog/hog_normalizer\n",
    "results_train_hog = results_train_hog/hog_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sample_combined = results_sample_pca+results_sample_hog\n",
    "results_train_combined = results_train_pca+results_train_hog\n",
    "results_train_combined[:,0]=1\n",
    "exp_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50_perc_mc_attack_log: 0.500\n",
      "50_perc_mc_attack_eps: 0.510\n",
      "50_perc_mc_attack_frac: 0.510\n",
      "successfull_set_attack_1: 0.000\n",
      "successfull_set_attack_2: 0.000\n",
      "successfull_set_attack_3: 0.000\n",
      "11_perc_mc_attack_log: 0.030\n",
      "11_perc_mc_attack_eps: 0.032\n",
      "11_perc_mc_attack_frac: 0.032\n"
     ]
    }
   ],
   "source": [
    "# save data\n",
    "new_row = np.zeros(1, dtype = dt)[0]\n",
    "new_row['instance_no'] = instance_no\n",
    "new_row['exp_no'] = exp_no\n",
    "new_row['method'] = 9 # bagging cat\n",
    "new_row['percentage_of_data'] = percentage\n",
    "new_row['percentile'] = -1\n",
    "new_row['mc_hog_no_batches'] = 0\n",
    "\n",
    "mc_attack_results = mc_attack(results_sample_combined, results_train_combined)\n",
    "new_row['50_perc_mc_attack_log'] = mc_attack_results[0]\n",
    "new_row['50_perc_mc_attack_eps'] = mc_attack_results[1]\n",
    "new_row['50_perc_mc_attack_frac'] = mc_attack_results[2]\n",
    "new_row['11_perc_mc_attack_log'] = mc_attack_results[3]\n",
    "new_row['11_perc_mc_attack_eps'] = mc_attack_results[4]\n",
    "new_row['11_perc_mc_attack_frac'] = mc_attack_results[5]\n",
    "    \n",
    "experiment_results.append(new_row)\n",
    "np.savetxt(experiment+'.csv', np.array(experiment_results, dtype = dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.80697418e-06, 1.61272856e-05,\n",
       "       2.55000009e-06, 6.48454534e-06, 3.88009270e-04, 3.46722627e-05,\n",
       "       2.18730667e-04, 5.29061363e-04, 2.16184650e-04, 4.34412126e-04,\n",
       "       2.17582332e-03, 3.45829688e-03, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.33426163e-05, 2.08910860e-05,\n",
       "       2.55000009e-06, 2.55000009e-06, 1.17684285e-05, 1.25850256e-05,\n",
       "       5.51903446e-04, 7.87692188e-05, 4.01575206e-04, 6.22994208e-04,\n",
       "       2.50948453e-03, 3.89148481e-03, 1.54095097e-03, 2.30619516e-02,\n",
       "       2.42588878e-01, 7.31650174e-01, 3.56461406e-01, 6.05099201e-01,\n",
       "       6.37995780e-01, 1.18955731e-01, 9.84425470e-03, 1.09355769e-03,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 1.92552070e-05, 5.73662692e-04,\n",
       "       1.81294163e-04, 1.16749457e-03, 7.25277569e-05, 2.41557616e-04,\n",
       "       5.20140748e-04, 9.69021930e-04, 7.78537104e-03, 4.31356160e-03,\n",
       "       1.76398471e-01, 3.10955763e-01, 2.15808702e+00, 2.53861809e+00,\n",
       "       2.59733448e+01, 8.58628006e+01, 6.83655548e+01, 8.41855469e+01,\n",
       "       1.11386993e+02, 3.42577910e+00, 7.07909822e-01, 4.49598521e-01,\n",
       "       3.84990610e-02, 7.18006550e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 5.88390867e-06, 5.93980003e-05,\n",
       "       2.55000009e-06, 5.54809580e-04, 1.54613995e-03, 1.82067591e-03,\n",
       "       2.03724522e-02, 5.37741277e-03, 1.15174679e-02, 7.70455003e-02,\n",
       "       3.25199664e-01, 7.33248651e-01, 2.47465992e+00, 1.66535187e+01,\n",
       "       1.09566422e+02, 1.97298920e+02, 1.95541656e+02, 1.59106979e+02,\n",
       "       7.87916718e+01, 1.46391172e+01, 1.14437997e+00, 6.37332439e-01,\n",
       "       2.75094267e-02, 4.22542635e-03, 8.52087542e-05, 7.78999529e-05,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       3.03824072e-05, 1.90230808e-03, 4.36093062e-02, 2.57128235e-02,\n",
       "       1.98924039e-02, 3.62341292e-02, 5.43212742e-02, 4.84488457e-02,\n",
       "       2.12498412e-01, 9.90694940e-01, 7.68043661e+00, 6.31825409e+01,\n",
       "       1.24451775e+02, 1.68630096e+02, 1.98926468e+02, 1.81870667e+02,\n",
       "       1.23974892e+02, 2.90268269e+01, 1.01418817e+00, 1.95285052e-01,\n",
       "       5.09968735e-02, 9.89860157e-04, 4.50752734e-04, 7.98931251e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 1.75174573e-05, 3.16915481e-04,\n",
       "       9.02335346e-03, 9.11165681e-03, 4.11605230e-03, 1.38372164e-02,\n",
       "       1.90441906e-02, 4.56814356e-02, 3.28023098e-02, 6.32837713e-02,\n",
       "       4.00901705e-01, 4.02923203e+00, 5.25464973e+01, 1.50249969e+02,\n",
       "       1.70858810e+02, 1.58140732e+02, 1.78523895e+02, 1.81131424e+02,\n",
       "       1.45433228e+02, 2.02364407e+01, 1.07463086e+00, 4.09715176e-02,\n",
       "       2.44500954e-02, 3.97248892e-04, 6.83103630e-04, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.57099055e-05, 1.86709749e-05, 1.74255492e-05,\n",
       "       2.78522693e-05, 1.27539714e-03, 8.11244361e-03, 1.43176466e-02,\n",
       "       2.43195053e-02, 2.62902379e-02, 5.12494147e-02, 3.50452781e-01,\n",
       "       4.63771677e+00, 7.00668945e+01, 1.82028915e+02, 1.84208160e+02,\n",
       "       1.58495621e+02, 1.16393257e+02, 1.11371361e+02, 1.46191040e+02,\n",
       "       1.22548309e+02, 2.62572651e+01, 6.60009742e-01, 7.21559748e-02,\n",
       "       4.43403386e-02, 5.82327694e-03, 7.25374324e-04, 3.41518789e-05,\n",
       "       2.55000009e-06, 1.45876938e-05, 4.27337045e-06, 5.20287322e-05,\n",
       "       1.18656753e-04, 3.45656229e-03, 1.51209589e-02, 3.18139605e-02,\n",
       "       2.46759839e-02, 9.01411027e-02, 9.15618420e-01, 6.97482014e+00,\n",
       "       6.24937859e+01, 1.86709213e+02, 2.04871338e+02, 1.49651474e+02,\n",
       "       6.60139847e+01, 3.81082191e+01, 7.98116379e+01, 1.55190460e+02,\n",
       "       1.11747444e+02, 2.74105339e+01, 1.05296111e+00, 1.59056172e-01,\n",
       "       9.71922800e-02, 8.04170966e-03, 2.25659949e-03, 1.49669386e-05,\n",
       "       2.55000009e-06, 2.55000009e-06, 5.84830814e-05, 5.96170139e-04,\n",
       "       3.31631105e-04, 2.19677575e-02, 3.71875428e-02, 6.20548390e-02,\n",
       "       6.49366155e-02, 7.13766813e-01, 1.26634808e+01, 6.86527863e+01,\n",
       "       1.56948914e+02, 1.77235107e+02, 1.10080498e+02, 2.51360111e+01,\n",
       "       8.40257645e+00, 1.08207045e+01, 3.57331505e+01, 1.31735931e+02,\n",
       "       1.40406830e+02, 4.21354828e+01, 2.79717231e+00, 3.28146160e-01,\n",
       "       6.48105592e-02, 3.99817666e-03, 1.97900203e-03, 1.48524053e-03,\n",
       "       2.55000009e-06, 3.94607268e-05, 2.15102955e-05, 1.75768786e-04,\n",
       "       3.97182116e-03, 2.08023004e-02, 2.16024742e-02, 7.72336274e-02,\n",
       "       4.29656029e-01, 1.57487459e+01, 1.16849243e+02, 1.95244156e+02,\n",
       "       1.78720825e+02, 6.63854599e+01, 1.95925865e+01, 5.68965292e+00,\n",
       "       2.44349527e+00, 3.49744749e+00, 1.99808960e+01, 9.76149826e+01,\n",
       "       1.68618103e+02, 6.53622513e+01, 3.19487524e+00, 1.85370073e-01,\n",
       "       2.16395650e-02, 3.71913542e-04, 1.52070119e-04, 4.36843766e-05,\n",
       "       2.55000009e-06, 2.43458780e-05, 1.30236085e-05, 1.28933997e-03,\n",
       "       3.96584161e-03, 5.59366774e-03, 3.74779515e-02, 3.73205513e-01,\n",
       "       6.43142319e+00, 1.07114136e+02, 2.20972610e+02, 2.03178314e+02,\n",
       "       9.83620453e+01, 1.28566008e+01, 2.82940507e+00, 1.10047734e+00,\n",
       "       8.26773167e-01, 1.54258597e+00, 8.91677570e+00, 7.41976166e+01,\n",
       "       2.01653519e+02, 1.02813850e+02, 4.83149338e+00, 2.23229572e-01,\n",
       "       2.53086239e-02, 6.67853164e-04, 2.89423933e-05, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 7.28959712e-05, 2.34248728e-04,\n",
       "       5.77418657e-04, 1.35076996e-02, 1.05119422e-01, 1.95411015e+00,\n",
       "       4.85205803e+01, 1.87628128e+02, 2.09203766e+02, 1.17469910e+02,\n",
       "       1.73959656e+01, 2.85815763e+00, 6.38742685e-01, 2.24420056e-01,\n",
       "       4.64753956e-01, 1.69163966e+00, 7.36456347e+00, 6.97117386e+01,\n",
       "       1.90779922e+02, 1.22052780e+02, 7.94220304e+00, 2.89887905e-01,\n",
       "       1.73834264e-02, 9.63210012e-04, 1.09466846e-05, 2.55000009e-06,\n",
       "       2.55000009e-06, 3.10463365e-05, 2.47409567e-04, 4.65415622e-04,\n",
       "       1.00221084e-02, 1.29822403e-01, 1.39484549e+00, 2.03065948e+01,\n",
       "       1.49756149e+02, 1.92310913e+02, 1.25340248e+02, 1.67986984e+01,\n",
       "       3.27101970e+00, 1.23810971e+00, 2.23971233e-01, 2.28657424e-01,\n",
       "       6.71465695e-01, 2.08403802e+00, 7.75860310e+00, 7.92428131e+01,\n",
       "       1.86116852e+02, 1.08600174e+02, 7.54044342e+00, 3.75882447e-01,\n",
       "       1.95695758e-02, 5.83480978e-05, 8.49081880e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 1.41644530e-04, 2.76996521e-04,\n",
       "       1.37861772e-02, 6.45456135e-01, 9.46116829e+00, 9.94957504e+01,\n",
       "       1.97907654e+02, 1.86924789e+02, 4.19981613e+01, 3.41986775e+00,\n",
       "       8.00922394e-01, 2.90140271e-01, 2.36299276e-01, 3.51036668e-01,\n",
       "       4.34890896e-01, 1.93657660e+00, 7.72095108e+00, 8.17906342e+01,\n",
       "       1.82554489e+02, 1.17166977e+02, 9.47447586e+00, 2.88036197e-01,\n",
       "       1.10248970e-02, 2.55000009e-06, 1.04507990e-05, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 8.91239251e-05, 5.19938534e-03,\n",
       "       3.23108882e-02, 2.09829307e+00, 3.02925224e+01, 1.62707840e+02,\n",
       "       1.81671753e+02, 1.00667664e+02, 7.25473976e+00, 4.70923454e-01,\n",
       "       1.40632436e-01, 1.55816838e-01, 2.34178737e-01, 4.20939863e-01,\n",
       "       6.63895249e-01, 2.63136196e+00, 1.73215542e+01, 1.06659386e+02,\n",
       "       1.77729614e+02, 6.95926514e+01, 2.89003301e+00, 7.75429085e-02,\n",
       "       2.30536354e-03, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.78688880e-04, 3.84285953e-03,\n",
       "       3.66777748e-01, 5.55412912e+00, 4.99981499e+01, 1.72830475e+02,\n",
       "       1.26155342e+02, 2.36376724e+01, 2.63690257e+00, 2.76589304e-01,\n",
       "       2.81697094e-01, 5.00479400e-01, 6.53978467e-01, 9.19170737e-01,\n",
       "       1.32721591e+00, 4.69151831e+00, 3.82295532e+01, 1.63352448e+02,\n",
       "       1.67012253e+02, 3.58242989e+01, 1.29109919e+00, 1.18375115e-01,\n",
       "       1.65534869e-03, 4.79381524e-05, 9.82790334e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 8.42553927e-05, 3.65410786e-04, 9.71643999e-03,\n",
       "       7.18112051e-01, 1.19946012e+01, 7.50969086e+01, 1.62924103e+02,\n",
       "       1.08537903e+02, 2.53613262e+01, 4.20573854e+00, 7.78565049e-01,\n",
       "       8.64229620e-01, 1.08150554e+00, 1.04095840e+00, 1.08547616e+00,\n",
       "       2.86538577e+00, 1.42729921e+01, 9.11632843e+01, 1.80932846e+02,\n",
       "       1.25976494e+02, 8.57418919e+00, 2.25506812e-01, 3.57247330e-02,\n",
       "       1.01299083e-03, 2.80778797e-04, 7.22966070e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 5.16049840e-05, 4.91533801e-03, 1.81107774e-01,\n",
       "       7.42088377e-01, 7.99089241e+00, 8.47964172e+01, 1.80178223e+02,\n",
       "       1.39705643e+02, 5.32733803e+01, 7.20520830e+00, 1.32432818e+00,\n",
       "       1.45348263e+00, 2.51470470e+00, 2.89650679e+00, 5.89757681e+00,\n",
       "       2.79261360e+01, 8.18347549e+01, 1.67122238e+02, 1.58636520e+02,\n",
       "       4.27879562e+01, 1.29832208e+00, 3.13206092e-02, 2.89014596e-02,\n",
       "       1.41648087e-03, 1.33874943e-04, 1.05052777e-05, 2.55000009e-06,\n",
       "       2.55000009e-06, 3.29909890e-05, 9.17472411e-03, 4.27108228e-01,\n",
       "       3.91760200e-01, 4.60430336e+00, 7.12197723e+01, 1.65701706e+02,\n",
       "       1.72033279e+02, 9.75754089e+01, 2.20609608e+01, 6.60475969e+00,\n",
       "       8.04315090e+00, 8.35935211e+00, 1.48150501e+01, 5.08346672e+01,\n",
       "       1.55604050e+02, 1.98746643e+02, 1.68967453e+02, 6.15844727e+01,\n",
       "       3.20670152e+00, 7.38735870e-02, 1.02515705e-02, 9.88361239e-03,\n",
       "       1.62247720e-03, 3.60631821e-05, 1.60132604e-05, 2.55000009e-06,\n",
       "       2.55000009e-06, 3.23906752e-05, 1.24350069e-02, 3.42030317e-01,\n",
       "       2.19628215e-01, 3.02433634e+00, 4.83306351e+01, 1.42287231e+02,\n",
       "       1.94900040e+02, 1.59192001e+02, 7.91551590e+01, 5.05547409e+01,\n",
       "       6.17355576e+01, 7.00218201e+01, 1.15707741e+02, 1.66571106e+02,\n",
       "       2.05475601e+02, 1.79169373e+02, 6.22514915e+01, 5.62919998e+00,\n",
       "       1.48596451e-01, 3.19341645e-02, 1.31204659e-02, 2.03877464e-02,\n",
       "       3.83211486e-03, 1.91432206e-04, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 9.79058072e-03, 5.01020476e-02,\n",
       "       8.06981847e-02, 6.66032493e-01, 1.57696257e+01, 9.50104904e+01,\n",
       "       1.87175461e+02, 1.91421616e+02, 1.82520386e+02, 1.88733246e+02,\n",
       "       1.80935562e+02, 1.92390366e+02, 2.04402084e+02, 2.01552780e+02,\n",
       "       1.58643387e+02, 5.43539200e+01, 6.72735167e+00, 2.39224613e-01,\n",
       "       1.51701961e-02, 1.76840015e-02, 1.40606044e-02, 1.95119958e-02,\n",
       "       4.63653734e-04, 3.82169710e-05, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.17407886e-02,\n",
       "       1.34194568e-02, 2.70867459e-02, 1.00299370e+00, 1.57288361e+01,\n",
       "       8.44408112e+01, 1.38475098e+02, 1.75907501e+02, 1.99933334e+02,\n",
       "       1.89068176e+02, 1.95907593e+02, 1.82176956e+02, 1.18742264e+02,\n",
       "       2.46008091e+01, 2.55815983e+00, 2.26721272e-01, 2.60771122e-02,\n",
       "       3.63417412e-03, 3.15265264e-03, 1.12306746e-02, 1.07917022e-02,\n",
       "       2.14876258e-03, 6.50007405e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 1.46270922e-05,\n",
       "       1.25402774e-04, 1.64505024e-03, 3.21467519e-02, 3.55503350e-01,\n",
       "       3.56217790e+00, 1.00981464e+01, 2.55625114e+01, 2.48803959e+01,\n",
       "       2.19454651e+01, 2.13949413e+01, 1.65227814e+01, 6.90827465e+00,\n",
       "       5.85723877e-01, 7.44904429e-02, 1.55766001e-02, 7.63059966e-03,\n",
       "       1.36983639e-03, 1.06839987e-03, 3.48841436e-02, 6.16520317e-03,\n",
       "       3.58113204e-04, 9.31446193e-05, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 4.74124590e-06,\n",
       "       5.10123664e-06, 2.06652294e-05, 1.75020759e-04, 1.03501638e-03,\n",
       "       4.83643590e-03, 4.57731681e-03, 2.52613379e-03, 1.90030260e-03,\n",
       "       1.42422889e-03, 3.66052915e-03, 2.67019472e-03, 1.43013056e-03,\n",
       "       1.77665584e-04, 1.93625398e-04, 2.68237258e-04, 1.38602380e-04,\n",
       "       8.46894400e-05, 2.87215284e-04, 1.81497913e-03, 1.76025314e-05,\n",
       "       3.31235242e-05, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       1.38432239e-04, 4.82551375e-04, 2.49612931e-04, 2.79960077e-04,\n",
       "       1.16810610e-03, 2.07504979e-03, 3.42281489e-03, 3.74752772e-03,\n",
       "       5.86665981e-03, 2.99427053e-03, 2.03481014e-03, 7.60366966e-04,\n",
       "       2.48953467e-04, 4.34304442e-04, 7.59151822e-04, 2.55445833e-04,\n",
       "       4.53638950e-06, 1.55643702e-05, 6.16297984e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 6.80304720e-06, 1.02003294e-04, 1.64411787e-04,\n",
       "       3.60093720e-04, 3.57625610e-03, 1.29126966e-01, 1.51450142e-01,\n",
       "       1.66188944e-02, 4.54285694e-03, 3.18147521e-03, 1.16965768e-03,\n",
       "       7.68225000e-04, 1.45868878e-04, 2.81406756e-05, 1.26595851e-05,\n",
       "       2.55000009e-06, 6.82880045e-06, 8.22936490e-05, 2.10522849e-05,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 7.48979210e-06,\n",
       "       6.19145867e-05, 3.11449076e-05, 2.38253997e-05, 5.10321879e-05,\n",
       "       1.34823451e-04, 2.50977209e-05, 2.11733368e-05, 1.56806098e-04,\n",
       "       2.23461047e-05, 1.20376553e-05, 2.55000009e-06, 2.55000009e-06,\n",
       "       2.55000009e-06, 9.25409222e-06, 1.64118701e-05, 2.55000009e-06,\n",
       "       2.55000009e-06, 2.55000009e-06, 2.55000009e-06, 2.55000009e-06],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.load('trX_vae.npy')*255)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 8, 9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('trX_vae.npy')\n",
    "np.load('trY_vae.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self, digit):\n",
    "        self.input_dim = dim_z\n",
    "        self.output_dim = 784\n",
    "        self.digit = digit\n",
    "\n",
    "    def __call__(self, z):\n",
    "        return self.generate_samples_tf(z, self.digit)\n",
    "\n",
    "    def generate_samples_tf(self, Z,digit):\n",
    "        \n",
    "        sample_size = tf.shape(Z)[0]\n",
    "        \n",
    "        one_hot_digits = np.zeros(10,)\n",
    "        one_hot_digits[digit] = 1\n",
    "        Y_np_sample = tf.reshape(tf.tile(tf.constant(one_hot_digits, dtype=tf.float32),[sample_size]), [sample_size, 10])\n",
    "        \n",
    "        #Z_np_sample = tf.cast(Z_np_sample, tf.float32)\n",
    "        #Y_np_sample = tf.cast(Y_np_sample, tf.float32)\n",
    "        generated_samples = vae.decoder(Z, Y_np_sample, dim_img, n_hidden)\n",
    "        \n",
    "        # transform to match validation data\n",
    "        generated_samples = tf.reshape(generated_samples, [tf.shape(generated_samples)[0], 784])\n",
    "\n",
    "        # cast to int\n",
    "        #generated_samples = tf.cast(generated_samples, tf.int32)\n",
    "\n",
    "        return generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\D065042\\OneDrive - SAP SE\\5. Praxisphase Bachelorarbeit\\Python\\Monte-Carlo-MNIST_CVAE\\parzen.py:13: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\D065042\\OneDrive - SAP SE\\5. Praxisphase Bachelorarbeit\\Python\\Monte-Carlo-MNIST_CVAE\\parzen.py:14: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(0.7309, array([-147728.], dtype=float32), array([-147728.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(trY[0])\n",
    "prior = NormalPrior()\n",
    "kernel = ParsenDensityEstimator()\n",
    "\n",
    "sigma = 0.025\n",
    "model = ais.Model(generator, prior, kernel, sigma, 10000)\n",
    "\n",
    "num_samples = 1\n",
    "schedule = ais.get_schedule(num_samples, rad=4)\n",
    "p2 = model.ais(np.array([trX[0]]), schedule)\n",
    "\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot(X, n=10, negative_class=0.):\n",
    "    X = np.asarray(X).flatten()\n",
    "    if n is None:\n",
    "        n = np.max(X) + 1\n",
    "    Xoh = np.ones((len(X), n)) * negative_class\n",
    "    Xoh[np.arange(len(X)), X] = 1.\n",
    "    return Xoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros((100,))\n",
    "y[:] = 1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "Z_np_sample_buffer = np.random.randn(len(Y_Labels), dim_z)\n",
    "Y_Labels = OneHot(Y_Labels)\n",
    "\n",
    "generated_samples = sess.run(decoded, feed_dict={z_in: Z_np_sample_buffer, fack_id_in: Y_Labels, keep_prob : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.4577275e-10, 4.8040927e-10, 5.2374305e-10, ..., 2.8561100e-09,\n",
       "        1.1276368e-09, 2.5707847e-09],\n",
       "       [4.1918931e-09, 3.5248532e-10, 1.2809082e-09, ..., 4.7559273e-10,\n",
       "        4.4434092e-09, 8.8464451e-11],\n",
       "       [4.7324999e-10, 1.4354905e-09, 9.5292008e-10, ..., 3.5966325e-09,\n",
       "        1.6841941e-09, 2.0631703e-10],\n",
       "       ...,\n",
       "       [4.5303254e-11, 2.5053085e-10, 2.6552930e-11, ..., 3.2445518e-11,\n",
       "        5.5640478e-11, 3.1044525e-11],\n",
       "       [4.5461968e-09, 2.6213141e-09, 3.0995495e-10, ..., 2.3530047e-09,\n",
       "        3.5507530e-09, 3.4395060e-09],\n",
       "       [3.3948438e-11, 2.4918551e-10, 3.1829028e-10, ..., 4.2411821e-10,\n",
       "        4.1339029e-11, 3.7154035e-10]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def save_image(data, path):\n",
    "    img = np.zeros((28, 28, 3))\n",
    "    img[0:28, 0:28, :] = data.reshape( [28, 28, 1])\n",
    "    scipy.misc.imsave('./'+path+'.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\d065042\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Y_Labels)):\n",
    "    save_image(generated_samples[i],'test'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes 1,11,21,31,... are ones, 2,12,22 are twos etc.\n",
    "def generate_samples_for_digits(sample_size=100):\n",
    "    \n",
    "    Z_np_sample_buffer = np.random.randn(sample_size, dim_z)\n",
    "    \n",
    "    digits = np.zeros((sample_size,)).astype(int)\n",
    "    for i in range(len(digits)):\n",
    "        digits[i] = i%10\n",
    "    Y_np_sample = OneHot( digits)\n",
    "\n",
    "    generated_samples = sess.run(decoded, feed_dict={z_in: Z_np_sample_buffer, fack_id_in: Y_np_sample, keep_prob : 1})\n",
    "    return generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = generate_samples_for_digits(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
